{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get OpenML data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import openml\n",
    "import scipy.io.arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = None\n",
    "TEMP_DIR = 'run_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEMP_DIR):\n",
    "    os.makedirs(TEMP_DIR)\n",
    "\n",
    "if API_KEY is not None:\n",
    "    apikey = API_KEY\n",
    "else:\n",
    "    apikey = os.environ.get('OPENML_APIKEY', None)\n",
    "    if apikey is None:\n",
    "        raise RuntimeError('OpenML API key was not provided.')\n",
    "openml.config.apikey = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "\n",
    "# Testcase\n",
    "# task_id, flow_id = 31, 5889  # 261, None\n",
    "task_id, flow_id = 31, 66\n",
    "\n",
    "##########     ##########     ##########     ##########     ##########\n",
    "\n",
    "# Top-performing pipeline for each data set (i.e., task)\n",
    "## Supervised Classification on credit-approval # Gradient Boosting\n",
    "# task_id, flow_id = 29, 12736\n",
    "\n",
    "## Supervised Classification on credit-g # Ranger Classifier\n",
    "# task_id, flow_id = 31, 6794\n",
    "\n",
    "## Supervised Classification on adult # Boosting with Decision Trees\n",
    "# task_id, flow_id = 7592, 6970\n",
    "\n",
    "##########     ##########     ##########     ##########     ##########\n",
    "\n",
    "# Compare the influence of the shape of decision boundaries\n",
    "# task_id, flow_id = 31, 1720  # weka.J48 (decision tree) <- 22\n",
    "# task_id, flow_id = 31, 66    # weka.IBk (k-nearest neighbours) <- 13\n",
    "# task_id, flow_id = 31, 1820  # weka.MultilayerPerceptron <- 12\n",
    "## task_id, flow_id = 31, 5920  # weka.MultilayerPerceptron <- xx\n",
    "# task_id, flow_id = 31, 70    # weka.SMO_PolyKernel (SVM) <- 15\n",
    "# task_id, flow_id = 31, 72    # weka.SMO_RBFKernel (SVM) <- 33\n",
    "# task_id, flow_id = 31, 1726  # weka.RandomForest <- 20\n",
    "\n",
    "################################################################################\n",
    "\n",
    "temp_dir = TEMP_DIR\n",
    "\n",
    "# def get_task_predictions(task_id, temp_dir, flow_id=None):\n",
    "task_path = os.path.join(temp_dir, f'{task_id}')\n",
    "if not os.path.exists(task_path):\n",
    "    os.makedirs(task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get run ids for the task\n",
    "# for task 31 / flow 66 it takes roughly 5 minutes\n",
    "runs = openml.runs.list_runs(task=[task_id])\n",
    "\n",
    "if flow_id is None:\n",
    "    runs_id = list(runs.keys())\n",
    "else:\n",
    "    runs_id = [i for i in runs if runs[i]['flow_id'] == flow_id]\n",
    "\n",
    "# get the task info\n",
    "task = openml.tasks.get_task(task_id)\n",
    "# task_data, task_labels = task.get_X_and_y()\n",
    "_, task_test_indices = task.get_train_test_split_indices()  # task_train_indices\n",
    "task_test_indices_sorted = np.sort(task_test_indices)\n",
    "\n",
    "# get dataset info\n",
    "dataset_object = openml.datasets.get_dataset(task.dataset_id)\n",
    "dataset_target_label = dataset_object.default_target_attribute\n",
    "dataset = dataset_object.get_data()[0]\n",
    "\n",
    "task_labels_test = dataset[dataset_target_label][task_test_indices_sorted].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs in this flow: 13\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of runs in this flow: {len(runs_id)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelise_pool = 8\n",
    "\n",
    "PARALLELISE = not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARALLELISE:\n",
    "    import multiprocessing\n",
    "\n",
    "    pool = multiprocessing.Pool(parallelise_pool)\n",
    "    pool.map(openml.runs.get_run, runs_id)\n",
    "else:\n",
    "    # Get predictions from every run of this task\n",
    "    # flows = []\n",
    "    runs_list = openml.runs.get_runs(runs_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_msg = ('The previous step must be (re-)run without parallelisation to '\n",
    "        'proceed.')\n",
    "assert bool(runs_list) and not PARALLELISE, _msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two step can either be parallelised or not\n",
    "PARALLELISE = not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARALLELISE:\n",
    "    # https://code.activestate.com/recipes/82234-importing-a-dynamically-generated-module/\n",
    "    prl_str = f'''\\\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "task_path = 'run_cache/{task_id}'\n",
    "\n",
    "def download(run):\n",
    "    url = run.predictions_url\n",
    "    id_ = run.run_id\n",
    "\n",
    "    save_path = os.path.join(task_path, f'{{id_}}.arff')\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        urllib.request.urlretrieve(url, save_path)\n",
    "    '''\n",
    "    with open('download.py', 'w') as f:\n",
    "        f.write(prl_str)\n",
    "else:\n",
    "    def download(run):\n",
    "        url = run.predictions_url\n",
    "        id_ = run.run_id\n",
    "\n",
    "        # flow_id = run.flow_id\n",
    "        # openml.flows.get_flow(flow_id)\n",
    "        # flows.append(flow_id)\n",
    "\n",
    "        save_path = os.path.join(task_path, f'{id_}.arff')\n",
    "\n",
    "        if not os.path.exists(save_path):\n",
    "            urllib.request.urlretrieve(url, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PARALLELISE:\n",
    "    import multiprocessing\n",
    "    import download as dwnd\n",
    "\n",
    "    pool = multiprocessing.Pool(8)\n",
    "    pool.map(dwnd.download, runs_list)\n",
    "else:\n",
    "    for run in runs_list:\n",
    "        download(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not True:\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_bar(prog, tot):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"[%-99s] %d%%\" % ('='*prog, tot))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================================================================================] 100%"
     ]
    }
   ],
   "source": [
    "# load each run\n",
    "predictions_per_flow = dict()\n",
    "labels_per_fold = dict()\n",
    "\n",
    "max_bar = len(runs_id)\n",
    "for i_bar, id_ in enumerate(runs_id):\n",
    "    flow_id = runs[id_]['flow_id']\n",
    "\n",
    "    load_path = os.path.join(task_path, f'{id_}.arff')\n",
    "\n",
    "    prog_ = int(100*i_bar/max_bar)  # 100\n",
    "    progress_bar(prog_, prog_)\n",
    "\n",
    "    arff = scipy.io.arff.loadarff(load_path)\n",
    "    predictions = arff[0]\n",
    "\n",
    "    if 'repeat' in predictions.dtype.fields:\n",
    "        if not np.all(predictions['repeat'] == 0):\n",
    "            raise RuntimeError('Not executed in a single repeat')\n",
    "    else:\n",
    "        raise RuntimeError('repeat column missing')\n",
    "\n",
    "    if 'fold' in predictions.dtype.fields:\n",
    "        folds = np.unique(predictions['fold'])\n",
    "    else:\n",
    "        raise RuntimeError('fold column missing')\n",
    "\n",
    "    for f in folds:\n",
    "        fold_predictions_index = np.where(predictions['fold'] == f)\n",
    "        fold_predictions = predictions[fold_predictions_index]\n",
    "\n",
    "        if 'row_id' in fold_predictions.dtype.fields:\n",
    "            test_row_id_ = np.sort(fold_predictions['row_id'].astype(np.int64))\n",
    "            if f == 0 and not np.all(test_row_id_ == task_test_indices_sorted):\n",
    "                raise RuntimeError('Test row id mismatch!')\n",
    "        else:\n",
    "            raise RuntimeError('row_id column missing')\n",
    "\n",
    "        if 'correct' in fold_predictions.dtype.fields:\n",
    "            correct = np.sort(fold_predictions[['row_id', 'correct']], order=['row_id']\n",
    "                )['correct']\n",
    "            # if not np.all(correct == task_labels_test.astype(correct.dtype)):\n",
    "            #     raise RuntimeError('Test labels mismatch!')\n",
    "            if f == 0:\n",
    "                comp = task_labels_test\n",
    "            else:\n",
    "                comp = dataset[dataset_target_label][test_row_id_].to_numpy()\n",
    "            if not np.all(correct == comp.astype(correct.dtype)):\n",
    "                raise RuntimeError('Test labels mismatch!')\n",
    "        elif 'truth' in fold_predictions.dtype.fields:\n",
    "            correct = np.sort(fold_predictions[['row_id', 'truth']], order=['row_id']\n",
    "                )['truth']\n",
    "            if f == 0:\n",
    "                comp = task_labels_test\n",
    "            else:\n",
    "                comp = dataset[dataset_target_label][test_row_id_].to_numpy()\n",
    "            if not np.all(correct == comp.astype(correct.dtype)):\n",
    "                raise RuntimeError('Test labels mismatch!')\n",
    "        else:\n",
    "            raise RuntimeError('correct/truth column missing')\n",
    "        #\n",
    "        if f not in labels_per_fold:\n",
    "            labels_per_fold[f] = correct\n",
    "        else:\n",
    "            assert np.all(labels_per_fold[f] == correct)\n",
    "\n",
    "        if 'prediction' in fold_predictions.dtype.fields:\n",
    "            predicted = np.sort(fold_predictions[['row_id', 'prediction']], order=['row_id']\n",
    "                )['prediction']\n",
    "            \n",
    "            if flow_id not in predictions_per_flow:\n",
    "                predictions_per_flow[flow_id] = dict()\n",
    "            #\n",
    "            if f in predictions_per_flow[flow_id]:\n",
    "                predictions_per_flow[flow_id][f].append(predicted)\n",
    "            else:\n",
    "                predictions_per_flow[flow_id][f] = [predicted]\n",
    "        else:\n",
    "            raise RuntimeError('prediction column missing')\n",
    "progress_bar(100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve processed data\n",
    "with open(os.path.join(task_path, f'{task_id}_{flow_id}.pickle'), 'wb') as f:\n",
    "    pickle.dump(dict(\n",
    "        runs=predictions_per_flow,\n",
    "        labels=labels_per_fold),\n",
    "        f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17d0b7fbfdf90f8a6c5868d6926326caed38bdcf8916e35231a206bfb0137fef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('openml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
